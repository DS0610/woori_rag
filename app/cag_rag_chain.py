# cag_rag_chain.py
"""
CAG + RAG í†µí•© ì²´ì¸ ëª¨ë“ˆ (LangChain ê¸°ë°˜)
- CAG HIT: ìºì‹œëœ ë‹µë³€ ì¦‰ì‹œ ë°˜í™˜
- CAG MISS: RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±
"""

import os
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€ (rag ëª¨ë“ˆ importìš©)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import Dict, Optional
from app.cag import CAGCache

# Elasticsearch ê¸°ë°˜ RAG ì»´í¬ë„ŒíŠ¸
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer

# LangChain ì»´í¬ë„ŒíŠ¸
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser


class CAGRAGChain:
    """
    CAG â†’ RAG Fallback ì²´ì¸ (LangChain ê¸°ë°˜)
    
    ì›Œí¬í”Œë¡œìš°:
    1. CAG ìºì‹œ ì¡°íšŒ (similarity >= threshold ì‹œ HIT)
    2. MISSì‹œ Elasticsearch ë¬¸ì„œ ê²€ìƒ‰
    3. ê²€ìƒ‰ ê²°ê³¼ ìˆìœ¼ë©´ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
    4. Dynamic Cacheì— ì €ì¥
    """

    def __init__(
        self,
        redis_host: str = "localhost",
        redis_port: int = 6379,
        es_host: str = "http://localhost:9200",
        es_index: str = "customs-docs-v1",
        embedding_model: str = "jhgan/ko-sroberta-multitask",
        ollama_base_url: str = "http://localhost:11434",
        ollama_model: str = "llama3.2:3b",
        cache_threshold: float = 0.85,
    ):
        # CAG ìºì‹œ ì´ˆê¸°í™”
        self.cag = CAGCache(
            redis_host=redis_host,
            redis_port=redis_port,
            force_recreate_index=False,
        )
        self.cache_threshold = cache_threshold

        # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        print("ğŸ”§ RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.es_client = Elasticsearch(es_host, verify_certs=False)
        self.es_index = es_index

        # Elasticsearch ì—°ê²° í™•ì¸
        if not self.es_client.ping():
            print("âš ï¸ Elasticsearch ì—°ê²° ì‹¤íŒ¨ - RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤")
        else:
            print("âœ… Elasticsearch ì—°ê²° ì„±ê³µ")

        # LangChain LLM ì´ˆê¸°í™”
        print("ğŸ”§ LangChain LLM ì´ˆê¸°í™” ì¤‘...")
        self.llm = ChatOllama(
            model=ollama_model,
            base_url=ollama_base_url,
            temperature=0,
            timeout=120,
        )
        print("âœ… LangChain ChatOllama ì—°ê²° ì„±ê³µ")

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ ê´€ì„¸ì²­ì˜ ê³µì‹ AI ì—ì´ì „íŠ¸ 'ì»¤ìŠ¤í…€-ë´‡'ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì˜¤ì§ ì œê³µë˜ëŠ” [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì§€ì‹œ ì‚¬í•­]
1. ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ê¸° ìœ„í•´, [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ìœ¼ì„¸ìš”.
2. ë‹µë³€ì€ ëª…í™•í•˜ê³ , ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
3. ë§Œì•½ [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì— ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš©ì´ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, ì œê³µëœ ìë£Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. ì ˆëŒ€ [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì„ì˜ì˜ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
5. [ë§¤ìš° ì¤‘ìš”] ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œë§Œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
- ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì˜ˆì˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
- ì œëª©ì—ëŠ” ## ë˜ëŠ” ### ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œí•˜ì„¸ìš”.
- ë‹¨ê³„ë³„ ì„¤ëª… ì‹œ 1. 2. 3. ë²ˆí˜¸ ëª©ë¡ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- í•­ëª© ë‚˜ì—´ ì‹œ - ë¶ˆë¦¿í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ê¸ˆì•¡ì´ë‚˜ ìˆ˜ì¹˜ëŠ” ê°•ì¡°í•´ì„œ í‘œì‹œí•˜ì„¸ìš”."""

        # LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        self.prompt_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(self.system_prompt),
            HumanMessagePromptTemplate.from_template("""[ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]
{context}
---
[ì§ˆë¬¸]
{question}""")
        ])

        # LangChain ì²´ì¸ êµ¬ì„± (LCEL)
        self.rag_chain = self.prompt_template | self.llm | StrOutputParser()

    def _retrieve_documents(self, query: str, top_k: int = 3) -> str:
        """Elasticsearchì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            query_vector = self.embedding_model.encode(query).tolist()
            knn_query = {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": top_k,
                "num_candidates": 10,
            }
            response = self.es_client.search(
                index=self.es_index,
                knn=knn_query,
                source=["source", "content"],
                size=top_k,
            )
            hits = response["hits"]["hits"]
            if not hits:
                return ""

            context = ""
            for i, hit in enumerate(hits):
                context += f"\n--- ë¬¸ì„œ {i+1} (ì¶œì²˜: {hit['_source']['source']}) ---\n"
                context += hit["_source"]["content"]
                context += "\n-----------------------------------\n"
            return context
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return ""

    def _generate_answer(self, query: str, context: str) -> str:
        """LangChain LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            # LangChain LCEL ì²´ì¸ ì‹¤í–‰
            answer = self.rag_chain.invoke({
                "context": context,
                "question": query
            })
            return answer.strip()
        except Exception as e:
            error_msg = str(e)
            if "Connection" in error_msg:
                return "âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
            elif "timeout" in error_msg.lower():
                return "âŒ ì‘ë‹µ ì‹œê°„ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                return f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"

    def invoke(self, inputs: Dict[str, str]) -> Dict[str, any]:
        """
        CAG â†’ RAG ì²´ì¸ ì‹¤í–‰
        
        Args:
            inputs: {"question": str}
        
        Returns:
            {"answer": str, "cache_hit": bool, "source": str}
        """
        question = inputs.get("question", "")
        if not question:
            return {"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "cache_hit": False, "source": "NONE"}

        # 1. CAG ìºì‹œ ì¡°íšŒ
        print(f"\nğŸ” CAG ìºì‹œ ì¡°íšŒ: {question[:30]}...")
        cached_answer = self.cag.check_cache(question, threshold=self.cache_threshold)

        if cached_answer:
            print("âš¡ CAG HIT - ìºì‹œëœ ë‹µë³€ ë°˜í™˜")
            return {"answer": cached_answer, "cache_hit": True, "source": "CAG"}

        # 2. RAG íŒŒì´í”„ë¼ì¸: ë¬¸ì„œ ê²€ìƒ‰
        print("ğŸ“š CAG MISS - RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        context = self._retrieve_documents(question)

        if not context:
            print("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "cache_hit": False,
                "source": "NONE",
            }

        # 3. LangChain LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        print("ğŸ¤– LangChain LLM ë‹µë³€ ìƒì„± ì¤‘...")
        answer = self._generate_answer(question, context)

        # 4. Dynamic Cache ì €ì¥ (ì—ëŸ¬ ì‘ë‹µì€ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        if not answer.startswith("âŒ"):
            self.cag.save_dynamic_cache(question, answer)
            print("ğŸ’¾ Dynamic Cache ì €ì¥ ì™„ë£Œ")
        else:
            print("âš ï¸ ì—ëŸ¬ ì‘ë‹µì€ ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ")

        return {"answer": answer, "cache_hit": False, "source": "RAG"}


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (FastAPI, Streamlitì—ì„œ ê³µìœ )
_chain_instance: Optional[CAGRAGChain] = None


def get_chain() -> CAGRAGChain:
    """CAGRAGChain ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chain_instance
    if _chain_instance is None:
        _chain_instance = CAGRAGChain()
    return _chain_instance
